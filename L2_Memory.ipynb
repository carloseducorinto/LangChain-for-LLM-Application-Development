{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carloseducorinto/LangChain-for-LLM-Application-Development/blob/main/L2_Memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a786c77c",
      "metadata": {
        "id": "a786c77c"
      },
      "source": [
        "# LangChain: Memory\n",
        "\n",
        "## Outline\n",
        "* ConversationBufferMemory\n",
        "* ConversationBufferWindowMemory\n",
        "* ConversationTokenBufferMemory\n",
        "* ConversationSummaryMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1297dcd5",
      "metadata": {
        "id": "1297dcd5"
      },
      "source": [
        "## ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install langchain langchain_community\n",
        "#!pip install openai\n",
        "#!pip install -U langchain-openai"
      ],
      "metadata": {
        "id": "KhIJOx3xWnEv"
      },
      "id": "KhIJOx3xWnEv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a1f518f5",
      "metadata": {
        "height": 132,
        "tags": [],
        "id": "a1f518f5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2301270b-7a69-40f8-9934-f840499b6ae9",
      "metadata": {
        "id": "2301270b-7a69-40f8-9934-f840499b6ae9"
      },
      "source": [
        "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ce0924a2-ce2d-44ba-a806-c1195720c237",
      "metadata": {
        "height": 251,
        "id": "ce0924a2-ce2d-44ba-a806-c1195720c237"
      },
      "outputs": [],
      "source": [
        "llm_model = \"gpt-3.5-turbo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "20ad6fe2",
      "metadata": {
        "height": 81,
        "tags": [],
        "id": "20ad6fe2"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88bdf13d",
      "metadata": {
        "height": 132,
        "tags": [],
        "id": "88bdf13d"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "db24677d",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "db24677d",
        "outputId": "736f10ad-01e2-454a-d8c7-f081070ef403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Cadu\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Cadu! It's nice to meet you. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi, my name is Cadu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cc3ef937",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "cc3ef937",
        "outputId": "fcddefc4-7fb0-4db7-b941-fb9d64248bbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Cadu\n",
            "AI: Hello Cadu! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1+1 equals 2. Is there anything else you would like to know?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "acf3339a",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "acf3339a",
        "outputId": "11347ed6-aef6-49a1-d66a-6201ae37b2d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Cadu\n",
            "AI: Hello Cadu! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: 1+1 equals 2. Is there anything else you would like to know?\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is Cadu.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2529400d",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2529400d",
        "outputId": "c243afb6-4d40-48c2-c408-2fcf6a10911e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi, my name is Cadu\n",
            "AI: Hello Cadu! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: 1+1 equals 2. Is there anything else you would like to know?\n",
            "Human: What is my name?\n",
            "AI: Your name is Cadu.\n"
          ]
        }
      ],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5018cb0a",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5018cb0a",
        "outputId": "88a923af-0ee8-47ce-8b16-1f1726db99a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi, my name is Cadu\\nAI: Hello Cadu! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1+1 equals 2. Is there anything else you would like to know?\\nHuman: What is my name?\\nAI: Your name is Cadu.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "14219b70",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "14219b70"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a36e9905",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "a36e9905"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "61631b1f",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61631b1f",
        "outputId": "242d68b4-f5f7-40d5-cdfb-0fe0b55c846b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi\n",
            "AI: What's up\n",
            "Human: Hi\n",
            "AI: What's up\n"
          ]
        }
      ],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a2fdf9ec",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2fdf9ec",
        "outputId": "e0dbe4fe-7678-43d3-eba3-a192e8e6794a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Hi\\nAI: What's up\"}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7ca79256",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "7ca79256"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "890a4497",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "890a4497",
        "outputId": "dab99db5-ca7d-4602-90c1-3b9a1070a74b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf98e9ff",
      "metadata": {
        "tags": [],
        "id": "cf98e9ff"
      },
      "source": [
        "## ConversationBufferWindowMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "66eeccc3",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "66eeccc3"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "3ea6233e",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "3ea6233e"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferWindowMemory(k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "dc4553fb",
      "metadata": {
        "height": 98,
        "tags": [],
        "id": "dc4553fb"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "6a788403",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a788403",
        "outputId": "a0a43d17-5028-4a76-876f-25f89cbfdea3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "4087bc87",
      "metadata": {
        "height": 132,
        "tags": [],
        "id": "4087bc87"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "4faaa952",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "4faaa952",
        "outputId": "787682e5-6650-47de-a6d5-52ff26004dd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Andrew\n",
            "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
            "Human: Hi, my name is Cadu\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Cadu! It's great to meet you. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi, my name is Cadu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "bb20ddaa",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "bb20ddaa",
        "outputId": "294b2303-4186-4510-b13e-62109781a088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Cadu\n",
            "AI: Hello Cadu! It's great to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1+1 equals 2. Is there anything else you would like to know?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "489b2194",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "489b2194",
        "outputId": "fe1a5e31-9736-471a-cebe-0f330d18cb61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: What is 1+1?\n",
            "AI: 1+1 equals 2. Is there anything else you would like to know?\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry, I do not have access to personal information such as your name. Is there anything else you would like to know?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2931b92",
      "metadata": {
        "id": "d2931b92"
      },
      "source": [
        "## ConversationTokenBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f6d063c",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "9f6d063c"
      },
      "outputs": [],
      "source": [
        "#!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "fb9020ed",
      "metadata": {
        "height": 64,
        "tags": [],
        "id": "fb9020ed"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "from langchain.llms import OpenAI\n",
        "llm = ChatOpenAI(temperature=0.0, model=llm_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "43582ee6",
      "metadata": {
        "height": 132,
        "tags": [],
        "id": "43582ee6"
      },
      "outputs": [],
      "source": [
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
        "memory.save_context({\"input\": \"AI is what?!\"},\n",
        "                    {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
        "                    {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
        "                    {\"output\": \"Charming!\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "284288e1",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "284288e1",
        "outputId": "55e2c89a-3e64-4f44-face-015dbb0c442d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'AI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ff55d5d",
      "metadata": {
        "id": "5ff55d5d"
      },
      "source": [
        "## ConversationSummaryMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "72dcf8b1",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "72dcf8b1"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "4a5b238f",
      "metadata": {
        "height": 285,
        "tags": [],
        "id": "4a5b238f"
      },
      "outputs": [],
      "source": [
        "# create a long string\n",
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\"\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
        "                    {\"output\": f\"{schedule}\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "2e4ecabe",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e4ecabe",
        "outputId": "e9be9e94-65d2-4792-a7ae-880705db055c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'System: The human and AI exchange greetings and discuss the schedule for the day, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI provides details and reminders for each event.'}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "6728edba",
      "metadata": {
        "height": 98,
        "tags": [],
        "id": "6728edba"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "9a221b1d",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "9a221b1d",
        "outputId": "c64ccb8c-68fb-4784-ad4f-47da65ce6702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human and AI exchange greetings and discuss the schedule for the day, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI provides details and reminders for each event.\n",
            "Human: What would be a good demo to show?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'For the meeting with the product team, a demo showcasing the latest features and improvements in our product would be great. For the LangChain project work, a demo highlighting the progress made in implementing the new language processing algorithms would be impressive. And for the lunch meeting with the customer interested in AI, a demo showcasing how our AI technology can streamline their business processes would be ideal. Let me know if you need any specific details or assistance with preparing the demos!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "conversation.predict(input=\"What would be a good demo to show?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "bb582617",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb582617",
        "outputId": "32cdee38-0920-4916-dd54-3bc5ecb8ba2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'System: The human and AI exchange greetings and discuss the schedule for the day, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI provides details and reminders for each event. The human asks what would be a good demo to show.\\nAI: For the meeting with the product team, a demo showcasing the latest features and improvements in our product would be great. For the LangChain project work, a demo highlighting the progress made in implementing the new language processing algorithms would be impressive. And for the lunch meeting with the customer interested in AI, a demo showcasing how our AI technology can streamline their business processes would be ideal. Let me know if you need any specific details or assistance with preparing the demos!'}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6709b89-182e-4b7b-b811-1584dda20652",
      "metadata": {
        "id": "c6709b89-182e-4b7b-b811-1584dda20652"
      },
      "source": [
        "Reminder: Download your notebook to you local computer to save your work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ba827aa",
      "metadata": {
        "height": 30,
        "id": "4ba827aa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c07922b",
      "metadata": {
        "height": 30,
        "id": "2c07922b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52696c8c",
      "metadata": {
        "height": 30,
        "id": "52696c8c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48690d13",
      "metadata": {
        "height": 30,
        "id": "48690d13"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85bba1f8",
      "metadata": {
        "height": 30,
        "id": "85bba1f8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96a62e89",
      "metadata": {
        "height": 30,
        "id": "96a62e89"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f953ad8d",
      "metadata": {
        "height": 30,
        "id": "f953ad8d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9bb2617",
      "metadata": {
        "height": 30,
        "id": "b9bb2617"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be1989bf",
      "metadata": {
        "height": 30,
        "id": "be1989bf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1f8dca6",
      "metadata": {
        "height": 30,
        "id": "b1f8dca6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9120949",
      "metadata": {
        "height": 30,
        "id": "e9120949"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55849fb5",
      "metadata": {
        "height": 30,
        "id": "55849fb5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94ae491b",
      "metadata": {
        "height": 30,
        "id": "94ae491b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7c3aec3",
      "metadata": {
        "height": 30,
        "id": "e7c3aec3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ae51a3f",
      "metadata": {
        "height": 30,
        "id": "5ae51a3f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06894001",
      "metadata": {
        "height": 30,
        "id": "06894001"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2fb674c",
      "metadata": {
        "height": 30,
        "id": "e2fb674c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5158f4c8",
      "metadata": {
        "height": 30,
        "id": "5158f4c8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cebe066",
      "metadata": {
        "height": 30,
        "id": "2cebe066"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65caee4d",
      "metadata": {
        "height": 30,
        "id": "65caee4d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04799230",
      "metadata": {
        "height": 30,
        "id": "04799230"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30795c10",
      "metadata": {
        "height": 30,
        "id": "30795c10"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ed59c4",
      "metadata": {
        "height": 30,
        "id": "08ed59c4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb43eee3",
      "metadata": {
        "height": 30,
        "id": "bb43eee3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}